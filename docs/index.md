# How to Detect if that Flappy Bird Knock-off is Actually A Trojan Selling Your Data to the Deep Web
### Also Known As 
### My 2016 Summer Internship


## The Problem

My research problem in summer 2016 was to explore a dataset of a full Android app code package and a binary label of whether the app is malware or not. The main goal was to figure out how to make the best classifier from the data. 

It was a very open-ended problem, and I ended up focusing on a a) a data science perspective of what markers signify malware and semantically how it all fits together and b) how to improve on other models in the research literature by making the classifier more suitable for practical application.

## Learning More about the Domain Area

I read academic research papers to get a better idea of the domain. That was a good way to gather in-depth information without dealing with proprietary boundaries or marketing. 

Two approaches stood out: 

One was the DREBIN program of 2014. By 2016 DREBIN was the gold standard of this domain. It was possible to use DREBIN on the Android OS itself. It would add 10 seconds of checking the newly downloaded Android app before making its prediction, a theoretically small price to pay for peace of mind (to computer security researchers, at least). It detected 94% of malware in its dataset with a false-positive rate of 1% and an unnamed false-negative rate (I would heavily favor reducing false positives to false negatives in this problem area too, considering it's impossible to un-share/sell your data once it is leaked). It also by far outperformed commercial virus scanners which did not specialize in Android apps. 

I decided to follow their approach of using a support vector machine to classify the apps. They made the feature space as-large-as-possible by taking features from the Android app manifest file and from parsing the Android app bytecode file that is the app's program logic itself.

The other program I found interesting was ICCDetector, which specialized even further in this problem domain. It specialized in inter-app communication to find apps that took advantage of general phone events to make their move, so to speak. ICCDetector also used a support vector machine.

## My Approach

Going into model building, I knew that support vector machines worked well for these type of problems. I knew I wanted to include features from the Android app manifest file, because it is a single file access with straighforward parsing. All the known virus families request certain permissions through the Android app manifest file. I also knew that I wanted my approach to be faster than ten seconds per app. My entire approach became stripping away features that added to the feature extraction runtime and seeing the impact on accuracy. 

## Semantic Findings 

I made my initial dataset with all possible intent (for inter-app communication) and requested permission keywords from the app manifest file. Each app was a datapoint with an array of binary values of if it contained that feature in the manifest file.

![Highest Validation Accuracy over Different Data Sources](https://github.com/persinammon/nyit-malware/docs/graph1.png)

This graph shows that requested permissions are stronger indicators of if an app is malicious. The model's validation accuracy for a dataset of only intents is above random chance, but not by much. The combined dataset has a higher accuracy than the permissions only dataset, so my gut assumption would be that there are only a few intent filters that matter as features and the rest are irrelevant. This is based on there only being a 1.2% accuracy bump, which predicted on the complete 1000 app dataset is equivalent to 12 more apps correctly classified, and the fact that each feature is a binary value. I do not have the dataset now, but I suspect that if I isolated those 12 new apps, created a histogram of all intents and permissions requested in the manifest files, and set a bar starting from all 12 apps to at the bare least 7 of the apps, I could find a few common intent filters and reduce the feature space size significantly.

## Comparing Different Machine Learning Algorithms 

![Validation Accuracy of Models Based on Different Algorithms on Same Data](https://github.com/persinammon/nyit-malware/docs/graph2.png)

A support vector machine using a cubic function to separate the two classes had the highest validation accuracy. 

![Confusion Matrix of Model with Highest Validation Accuracy](https:://github.com/persinammon/nyit-malware/docs/graph3.png)

When I look further into the predictions of the cubic SVM, I see that the false negative accuracy is 10.8%. The probability is still similar to the overall miss rate of 8.3%, but I would enjoy hearing other comments about things to do in this situation. My original thoughts are to be risk-averse and make a new accuracy score that includes the correct predictions + incorrect positive predictions / total predictions (it's confusing, but incorrect positive = benign but predicted as malicious).

Knowing what I do now, I would use a linear discriminant algorithm on the data before feeding it into the support vector machines and k-Nearest Neighbor algorithms, instead of comparing its performance to the other algorithms as it is. 


## Impact

This big picture of this project is that it that it is possible to strip the program logic parsing of an Android app and still have a high accuracy in determining if it is malicious. This project's approach is technically more fool-proof than the approaches I described from the literature. Because it did not even look at parts of the app that could be obfuscated, this approach is invulnerable to code obfuscation, which the others could be fooled by. With work, I believe it is a commercializable idea as an open-sourced Android app for people on the Android app marketplace who want to avoid data breaches.

Answering the title, will this ML model be able to tell if that Flappy bird knock-off is a secret scam? Nine times out of ten, yes. 

## Helpful Resources

I found this [article (link stable on 8/9/21)](https://medium.com/androiddevnotes/the-internals-of-android-apk-build-process-article-5b68c385fb20) useful for understanding the Android app file structure.

I think another interesting avenue would be checking the src code versus *.dex code, because one of the other approaches saved time by parsing the *.dex code. A question to Android app developers would be: could an Android app can recompile its *.dex code based on the src code in the *.apk on your phone? Because that would make the previous programs obsolete.

If you found this writeup interesting, you can [fork my repo](https://github.com/persinammon/nyit-malware), follow some of the documentation, and try it on your own labeled malware dataset. The feature keywords in the script will save you time. Make sure to star the repo if you find it useful!