# How to Detect if that Flappy Bird Knock-off is Actually A Trojan Horse Malware Selling Your Data Wholesale on the Deep Web
### Also Known As 
### My 2016 Summer Internship
## By Monica Kumaran
[LinkedIn](https://www.linkedin.com/in/monica-kumaran-a93844205/) | 
 [About Me Page](https://www.monicakumaran.ink)


## The Problem

It's the Summer of 2016. At my summer undergraduate research experience I am given a dataset of a full Android app *.apk files with binary labels of whether the app is malware or not. The goal is to figure out how to make the best classifier from the data. 

It was a very open-ended problem. I ended up focusing on a a) a data science perspective of what markers signify malware and semantically how it all fits together and b) how to make a classifier that improves on the projects I read about.

## Learning More about the Domain Area

I read academic research papers to get a better idea of the domain. It was a good way to gather in-depth information without dealing with proprietary boundaries or marketing. My final deliverable was to write a conference paper on my project, so it also provided resources to write a literature review. 

Two approaches stood out: 

One was the DREBIN program of 2014. By 2016 DREBIN was the gold standard of this domain. It is possible to use DREBIN on the Android OS itself. It takes 10 seconds to check the newly downloaded Android app before making its prediction, a theoretically small price to pay for peace of mind (to computer security researchers, at least). It detects 94% of malware in its dataset with a false-positive rate of 1% and an unnamed false-negative rate. It also by far outperformed commercial virus scanners which did not specialize in Android apps. 

I decided to model my program after their approach. They made a feature space by taking features from the Android app manifest file and from parsing the Android app bytecode file that is the app's program logic and ran the feature data through a linear support vector machine.

The other program I found interesting was ICCDetector, which used features even more specialized than DREBIN. It specialized in inter-app communication to find apps that took advantage of general phone events to make their move and launch an attack or get root access to the phone. ICCDetector also used a support vector machine.

## My Approach

Going into model building, I knew that support vector machines worked well for these type of problems. I knew I wanted to include features from the Android app manifest file, because it is a single file access with straighforward parsing. All the known virus families request  **permissions** through the Android app manifest file, like permission to access certain data like Photos or the hardware like the camera, for examples. I could also include **intents** used for inter-app communication by extracting data from the manifest file. I also knew that I wanted my approach to be faster than ten seconds per app. My entire approach became stripping away features that added to runtime and seeing the impact on accuracy. 

## Semantic Findings 

I made my initial dataset with all possible requested intents and permission keywords from the app manifest file. Each app was a datapoint with an array of binary values of if it contained that feature in the manifest file.

![Highest Validation Accuracy over Different Data Sources](docs/graph1.PNG)

I checked the different types of features from the manifest file dataset to improve my intuition about this problem. This graph shows that requested permissions are stronger indicators of if an app is malicious. The model's validation accuracy for a dataset of only intents is above informationless random chance, but not by much. The model trained on the combined dataset had a higher accuracy than the one trained on the permissions only dataset, so my gut assumption would be that there are only a few intent filters that matter as features and the rest are irrelevant. 

My gut assumption is based on there only being a 1.2% accuracy bump, which predicted on the complete 1000 app dataset is equivalent to 12 more apps correctly classified, and the fact that each feature is a binary value. I do not have the dataset now, but I suspect that if I isolated those 12 new apps, created a histogram of all intents and permissions requested in the manifest files, and set a bar starting from all 12 apps and decreasing it to a minimum of 7 of the apps, I could find a few common intent filters between the malicious apps or between the benign apps. I could use what I learn to take out the features that don't affect the classification and save some time and memory by reducing the feature space.

## Comparing Different Machine Learning Algorithms 

![Validation Accuracy of Models Based on Different Algorithms on Same Data](https://github.com/persinammon/nyit-malware/blob/fe1a99c49b5ecc5c1f134e346da41dc6d4342bbf/docs/graph2.PNG)

A support vector machine using a cubic function to separate the two classes had the highest validation accuracy. 

![Confusion Matrix of Model with Highest Validation Accuracy](https://github.com/persinammon/nyit-malware/blob/fe1a99c49b5ecc5c1f134e346da41dc6d4342bbf/docs/graph3.PNG)

When I look further into the predictions of the cubic SVM, I see that the false negative accuracy is 10.8%. The probability is still similar to the overall miss rate of 8.3%, but I would enjoy hearing comments about things to do in this situation. My original thoughts are to be risk-averse and make a new accuracy score that includes the correct predictions + incorrect positive predictions / total predictions (it's confusing, but incorrect positive = benign but predicted as malicious). However, if this model were used commercially, it would depend on how much the market values accuracy over other tradeoffs.

Knowing what I do now, I would use the linear discriminant algorithm on the data before feeding it into the support vector machines and k-Nearest Neighbor algorithms. 


## Impact

This big takeaways from this data science project is that it that it is possible to only use the Android app manifest file to train a model and still have a high accuracy. This project's approach is technically more fool-proof than the approaches I described from the literature. Because it did not even look at parts of the app that could be obfuscated, this approach is invulnerable to code obfuscation, which the other two could be fooled by. The runtime would also be less because it skips a second step needed after disassembling the *.apk file. It only requires parsing an *.xml file, which is simple with libraries already available to use.

Answering the title, will this ML model be able to tell if that Flappy bird knock-off is a secret deep web scam? Nine times out of ten, yes. 

## Helpful Resources

I found this [article (link stable on 8/9/21)](https://medium.com/androiddevnotes/the-internals-of-android-apk-build-process-article-5b68c385fb20) useful for keeping track of the Android app file structure.

If you found this writeup useful for a similar project, you can [fork or look at the scripts in my repo](https://github.com/persinammon/nyit-malware) for feature extraction reference. Make sure to star it if you find it useful!
